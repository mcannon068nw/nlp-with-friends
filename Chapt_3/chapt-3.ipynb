{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is POS Tagging?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noun, plural'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explain can be used to report part of speech tag\n",
    "spacy.explain(\"NNS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('flowers', 'NNS', 'noun, plural')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"I saw flowers.\")\n",
    "token = doc[2]\n",
    "token.text, token.tag_, spacy.explain(token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alicia PROPN NNP proper noun noun, proper singular\n",
      "and CCONJ CC coordinating conjunction conjunction, coordinating\n",
      "me PRON PRP pronoun pronoun, personal\n",
      "went VERB VBD verb verb, past tense\n",
      "to ADP IN adposition conjunction, subordinating or preposition\n",
      "the DET DT determiner determiner\n",
      "school NOUN NN noun noun, singular or mass\n",
      "by ADP IN adposition conjunction, subordinating or preposition\n",
      "bus NOUN NN noun noun, singular or mass\n",
      ". PUNCT . punctuation punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "# Example of POS tagging in action\n",
    "doc = nlp(\"Alicia and me went to the school by bus.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.tag_, spacy.explain(token.pos_), spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My PRON PRP$ pronoun pronoun, possessive\n",
      "friend NOUN NN noun noun, singular or mass\n",
      "will AUX MD auxiliary verb, modal auxiliary\n",
      "fly VERB VB verb verb, base form\n",
      "to ADP IN adposition conjunction, subordinating or preposition\n",
      "New PROPN NNP proper noun noun, proper singular\n",
      "York PROPN NNP proper noun noun, proper singular\n",
      "fast ADV RB adverb adverb\n",
      "and CCONJ CC coordinating conjunction conjunction, coordinating\n",
      "she PRON PRP pronoun pronoun, personal\n",
      "is AUX VBZ auxiliary verb, 3rd person singular present\n",
      "staying VERB VBG verb verb, gerund or present participle\n",
      "there ADV RB adverb adverb\n",
      "for ADP IN adposition conjunction, subordinating or preposition\n",
      "3 NUM CD numeral cardinal number\n",
      "days NOUN NNS noun noun, plural\n",
      ". PUNCT . punctuation punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "# Another example of POS tagging in action\n",
    "doc = nlp(\"My friend will fly to New York fast and she is staying there for 3 days.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.tag_, spacy.explain(token.pos_),spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EX 1:  I PRP pronoun, personal\n",
      "EX 1:  will MD verb, modal auxiliary\n",
      "EX 1:  ship VB verb, base form\n",
      "EX 1:  the DT determiner\n",
      "EX 1:  package NN noun, singular or mass\n",
      "EX 1:  tomorrow NN noun, singular or mass\n",
      "EX 2:  I PRP pronoun, personal\n",
      "EX 2:  saw VBD verb, past tense\n",
      "EX 2:  a DT determiner\n",
      "EX 2:  red JJ adjective (English), other noun-modifier (Chinese)\n",
      "EX 2:  ship NN noun, singular or mass\n",
      "EX 2:  . . punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "# One more example of POS tagging in action (with context dependent POS verb/noun)\n",
    "doc = nlp(\"I will ship the package tomorrow\")\n",
    "for token in doc:\n",
    "    print('EX 1: ', token.text, token.tag_, spacy.explain(token.tag_))\n",
    "\n",
    "doc_2 = nlp(\"I saw a red ship.\")\n",
    "for token in doc_2:\n",
    "    print('EX 2: ', token.text, token.tag_, spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My PRON PRP$ pronoun pronoun, possessive\n",
      "cat NOUN NN noun noun, singular or mass\n",
      "will AUX MD auxiliary verb, modal auxiliary\n",
      "fish VERB VB verb verb, base form\n",
      "for ADP IN adposition conjunction, subordinating or preposition\n",
      "a DET DT determiner determiner\n",
      "fish NOUN NN noun noun, singular or mass\n",
      "tomorrow NOUN NN noun noun, singular or mass\n",
      "in ADP IN adposition conjunction, subordinating or preposition\n",
      "a DET DT determiner determiner\n",
      "fishy ADJ JJ adjective adjective (English), other noun-modifier (Chinese)\n",
      "way NOUN NN noun noun, singular or mass\n",
      ". PUNCT . punctuation punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "# But what about tricky sentences?\n",
    "doc = nlp(\"My cat will fish for a fish tomorrow in a fishy way.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.tag_, spacy.explain(token.pos_), spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verb Tense & Aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[('flying', 'fly')]\n",
      "[('fly', 'fly')]\n"
     ]
    }
   ],
   "source": [
    "# Print words from docs if word POS is a present progressive verb or base/infinitive verb\n",
    "sent1 = \"I flew to Rome.\"\n",
    "sent2 = \"I'm flying to Rome.\"\n",
    "sent3 = \"I will fly to Rome.\"\n",
    "\n",
    "doc1 = nlp(sent1)\n",
    "doc2 = nlp(sent2)\n",
    "doc3 = nlp(sent3)\n",
    "\n",
    "for doc in [doc1, doc2, doc3]:\n",
    "    print([(w.text, w.lemma_) for w in doc if w.tag_== 'VBG' or w.tag_ == 'VB'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number, symbol, and punctuation tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He PRP pronoun, personal\n",
      "earned VBD verb, past tense\n",
      "$ $ symbol, currency\n",
      "5.5 CD cardinal number\n",
      "million CD cardinal number\n",
      "in IN conjunction, subordinating or preposition\n",
      "2020 CD cardinal number\n",
      "and CC conjunction, coordinating\n",
      "paid VBD verb, past tense\n",
      "% NN noun, singular or mass\n",
      "35 CD cardinal number\n",
      "tax NN noun, singular or mass\n",
      ". . punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"He earned $5.5 million in 2020 and paid %35 tax.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.tag_, spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue compound\n",
      "flower ROOT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"blue flower\")\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 95 nsubj\n",
      "counted 100 ROOT\n",
      "white 84 amod\n",
      "sheep 92 dobj\n",
      ". 97 punct\n"
     ]
    }
   ],
   "source": [
    "# ROOT token is only one without a parent\n",
    "doc = nlp(\"I counted white sheep.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRP nsubj counted\n",
      "counted VBD ROOT counted\n",
      "white JJ amod sheep\n",
      "sheep NN dobj counted\n",
      ". . punct counted\n"
     ]
    }
   ],
   "source": [
    "# Token.head can be used to identify dependency heads\n",
    "doc = nlp(\"I counted white sheep.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.tag_, token.dep_, token.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We PRP nsubj trying\n",
      "are VBP aux trying\n",
      "trying VBG ROOT trying\n",
      "to TO aux understand\n",
      "understand VB xcomp trying\n",
      "the DT det difference\n",
      "difference NN dobj understand\n",
      ". . punct trying\n"
     ]
    }
   ],
   "source": [
    "# Xcomp relation o a verb is a clause without its own subject (open complement)\n",
    "doc = nlp(\"We are trying to understand the difference.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.tag_, token.dep_, token.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queen NNP compound Katherine\n",
      "Katherine NNP nsubj died\n",
      ", , punct Katherine\n",
      "who WP nsubj was\n",
      "was VBD relcl Katherine\n",
      "the DT det mother\n",
      "mother NN attr was\n",
      "of IN prep mother\n",
      "Mary NNP compound Tudor\n",
      "Tudor NNP pobj of\n",
      ", , punct Katherine\n",
      "died VBD ROOT died\n",
      "at IN prep died\n",
      "1536 CD pobj at\n",
      ". . punct died\n"
     ]
    }
   ],
   "source": [
    "# Observing relations within sentences with subsentences\n",
    "doc = nlp(\"Queen Katherine, who was the mother of Mary Tudor, died at 1536.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.tag_, token.dep_, token.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Tagger (NER)\n",
    "A named entity is a real world object that we can refer to by a proper name or quantity of interest. It can be a person, a place, an organization, a company, a product, dates, times, percentages, monetary amounts, a drug, or a disease name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Donald Trump, France)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"The president Donald Trump visited France.\")\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(383,\n",
       " 'Companies, agencies, institutions, etc.',\n",
       " 'Companies, agencies, institutions, etc.')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"He worked for NASA.\")\n",
    "token = doc[3]\n",
    "token.ent_type, spacy.explain(token.ent_type_), spacy.explain(\"ORG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Albert Einstein, Ulm, 1879, ETH Zurich)\n",
      "Albert PERSON People, including fictional\n",
      "Einstein PERSON People, including fictional\n",
      "was  None\n",
      "born  None\n",
      "in  None\n",
      "Ulm GPE Countries, cities, states\n",
      "on  None\n",
      "1879 DATE Absolute or relative dates or periods\n",
      ".  None\n",
      "He  None\n",
      "studied  None\n",
      "electronical  None\n",
      "engineering  None\n",
      "at  None\n",
      "ETH ORG Companies, agencies, institutions, etc.\n",
      "Zurich ORG Companies, agencies, institutions, etc.\n",
      ".  None\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Albert Einstein was born in Ulm on 1879. He studied electronical engineering at ETH Zurich.\")\n",
    "print(doc.ents)\n",
    "for token in doc:\n",
    "    print(token.text, token.ent_type_, spacy.explain(token.ent_type_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Jean-Michel Basquiat, American, Haitian, Puerto Rican)\n",
      "Jean-Michel Basquiat PERSON People, including fictional\n",
      "American NORP Nationalities or religious or political groups\n",
      "Haitian GPE Countries, cities, states\n",
      "Puerto Rican NORP Nationalities or religious or political groups\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Jean-Michel Basquiat was an American artist of Haitian and Puerto Rican descent who gained fame with his graffiti and street art work\")\n",
    "print(doc.ents)\n",
    "for ent in doc.ents:\n",
    "    print(ent, ent.label_,spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m6/b6y4g9114836jky8p81w12mchscvrj/T/ipykernel_32487/1793895529.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mny_art\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.nytimes.com/2021/01/12/opinion/trump-america-allies.html\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_md\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/m6/b6y4g9114836jky8p81w12mchscvrj/T/ipykernel_32487/1793895529.py\u001b[0m in \u001b[0;36murl_text\u001b[0;34m(url_string)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html5lib'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mscript\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"script\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"style\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aside'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mbuilder_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuilder_class\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m                 raise FeatureNotFound(\n\u001b[0m\u001b[1;32m    246\u001b[0m                     \u001b[0;34m\"Couldn't find a tree builder with the features you \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                     \u001b[0;34m\"requested: %s. Do you need to install a parser library?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import spacy\n",
    "\n",
    "def url_text(url_string):\n",
    "    res = requests.get(url_string)\n",
    "    html = res.text\n",
    "    soup = BeautifulSoup(html, 'html5lib')\n",
    "    for script in soup([\"script\",\"style\", 'aside']):\n",
    "        script.extract()\n",
    "    text = soup.get_text()\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "ny_art = url_text(\"https://www.nytimes.com/2021/01/12/opinion/trump-america-allies.html\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "doc = nlp(ny_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "labels = [ent.label_ for ent in doc.ents]\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging and splitting tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(New Hampshire,)\n",
      "[('She', 0), ('lived', 1), ('in', 2), ('New', 3), ('Hampshire', 4), ('.', 5)]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"She lived in New Hampshire.\")\n",
    "print(doc.ents)\n",
    "print([(token.text, token.i) for token in doc])\n",
    "print(len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(doc[3:5], attrs={\"LEMMA\": \"new hampshire\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(New Hampshire,)\n",
      "[('She', 0), ('lived', 1), ('in', 2), ('New Hampshire', 3), ('.', 4)]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(doc.ents)\n",
    "print([(token.text, token.i) for token in doc])\n",
    "print(len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['she', 'live', 'in', 'new hampshire', '.']\n"
     ]
    }
   ],
   "source": [
    "print([(token.lemma_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[('She', 'she', 0), ('lived', 'live', 1), ('in', 'in', 2), ('NewHampshire', 'NewHampshire', 3)]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"She lived in NewHampshire\")\n",
    "print(len(doc))\n",
    "print([(token.text, token.lemma_, token.i) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E117] The newly split tokens must match the text of the original token. New orths: NewHampshire. Old text: New.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m6/b6y4g9114836jky8p81w12mchscvrj/T/ipykernel_32487/397916462.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mheads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"TAG\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"NNP\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"NNP\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"DEP\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"compound\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"pobj\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mretokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"New\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Hampshire\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/spacy/tokens/_retokenize.pyx\u001b[0m in \u001b[0;36mspacy.tokens._retokenize.Retokenizer.split\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E117] The newly split tokens must match the text of the original token. New orths: NewHampshire. Old text: New."
     ]
    }
   ],
   "source": [
    "with doc.retokenize() as retokenizer:\n",
    "    heads = [(doc[3], 1), doc[2]]\n",
    "    attrs = {\"TAG\": [\"NNP\", \"NNP\"],\"DEP\": [\"compound\",\"pobj\"]}\n",
    "    retokenizer.split(doc[3], [\"New\", \"Hampshire\"], heads=heads, attrs=attrs)\n",
    "\n",
    "print([(token.text, token.lemma_, token.i) for token in doc])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
